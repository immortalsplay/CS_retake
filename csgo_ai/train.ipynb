{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il4XGp-Euxg1"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from newdataset import CSGODataset, SmallCSGODataset\n",
        "# from csgo_dataset import CSGODataset, SmallCSGODataset\n",
        "from temporal import CSGO_model\n",
        "from newconfig import Config\n",
        "\n",
        "mouse_x_intervals = [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8,\n",
        "                     9, 10]\n",
        "mouse_y_intervals = [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8,\n",
        "                     9, 10]\n",
        "\n",
        "\n",
        "def contains_epoch_number(filename):\n",
        "    parts = filename.split('_')\n",
        "    if len(parts) > 1 and parts[-2] == 'epoch' and parts[-1].split('.')[0].isdigit():\n",
        "        return True\n",
        "    return False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# make dataset\n",
        "cs_go_dataset = CSGODataset(Config[\"file_path\"])\n",
        "small_cs_go_dataset = SmallCSGODataset(Config[\"file_path\"], num_samples=282,frames_per_sample=Config[\"horizon\"])\n",
        "# make DataLoader\n",
        "small_data_loader = DataLoader(small_cs_go_dataset, batch_size=Config[\"batch_size\"], shuffle=True)\n",
        "# init model\n",
        "model = CSGO_model(horizon = Config[\"horizon\"],\n",
        "                   num_feature=Config[\"num_feature\"],\n",
        "                   depth = Config[\"depth\"],\n",
        "                   num_heads = Config[\"num_heads\"],\n",
        "                   head_dim = Config[\"head_dim\"],\n",
        "                   inverse_dynamic_dim = Config[\"inverse_dynamic_dim\"],\n",
        "                   layer_norm_cfg=Config[\"layer_norm_cfg\"],\n",
        "                   model_option=Config[\"model_option\"],\n",
        "                   frame_count=Config[\"frame_count\"])\n",
        "# move to GPU\n",
        "model.to(device)\n",
        "# set optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "# set epoch\n",
        "start_epoch = 0\n",
        "save_freq = 5   # step 5\n",
        "best_loss = float('inf')\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_path = os.path.join('new_checkpoints',\n",
        "                           f'layer_norm_cfg_{Config[\"layer_norm_cfg\"]}',\n",
        "                           f'model_option_{Config[\"model_option\"]}',\n",
        "                           f'frame_count_{Config[\"frame_count\"]}')\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "checkpoint_files = deque(maxlen=3)\n",
        "\n",
        "latest_checkpoint = None  # Initialize latest_checkpoint to None\n",
        "\n",
        "# Check for \"best_model.pth\" first\n",
        "\n",
        "checkpoint_path = os.path.join('new_checkpoints',\n",
        "                           f'layer_norm_cfg_{Config[\"layer_norm_cfg\"]}',\n",
        "                           f'model_option_{Config[\"model_option\"]}',\n",
        "                           f'frame_count_{Config[\"frame_count\"]}')\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# Check for \"best_model.pth\" first\n",
        "best_model_path = os.path.join(checkpoint_path, 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    latest_checkpoint = 'best_model.pth'\n",
        "else:\n",
        "    # If \"best_model.pth\" does not exist, find the latest checkpoint\n",
        "    if os.listdir(checkpoint_path):\n",
        "        filtered_files = list(filter(contains_epoch_number, os.listdir(checkpoint_path)))\n",
        "        if filtered_files:\n",
        "            latest_checkpoint = max(filtered_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "if latest_checkpoint is not None:  # Only load the checkpoint if it exists\n",
        "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
        "    checkpoint = torch.load(os.path.join(checkpoint_path, latest_checkpoint))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    print(\"No valid checkpoint files found, starting from scratch.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combined_loss(batch_size,outputs, targets):\n",
        "    mouse_x_outputs ,mouse_y_outputs,other_outputs = outputs\n",
        "\n",
        "\n",
        "    mouse_x_loss=0\n",
        "    mouse_y_loss=0\n",
        "    other_loss=0\n",
        "    batch_size = outputs[0].shape[0]\n",
        "\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        mouse_x_loss += nn.NLLLoss()(mouse_x_outputs[b], targets[b, 0])\n",
        "        mouse_y_loss += nn.NLLLoss()(mouse_y_outputs[b], targets[b, 1])\n",
        "        other_loss += nn.BCELoss()(other_outputs[b, :], targets[b,  2:].float().unsqueeze(1))\n",
        "\n",
        "    return mouse_x_loss + mouse_y_loss + other_loss\n",
        "\n",
        "\n",
        "# TensorBoard\n",
        "writer = SummaryWriter()\n",
        "# Training\n",
        "model.train()\n",
        "n_epochs=Config[\"num_epochs\"]\n",
        "losses = []\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "    epoch_loss = 0\n",
        "    pbar = tqdm(enumerate(small_data_loader), total=len(small_data_loader), desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "    for batch_idx, (data, label) in pbar:\n",
        "        data = data.to(device)\n",
        "        # print(\"data.shape\",data.shape)\n",
        "        # print(\"data\",data)\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        # Loss\n",
        "        loss = combined_loss(Config[\"batch_size\"],outputs, label) # 用 combined_loss 替代之前的损失计算方式\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "    avg_loss = epoch_loss / len(small_data_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
        "    losses.append(avg_loss)\n",
        "    scheduler.step(avg_loss)\n",
        "    # TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "    # Save best model\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "             'optimizer_state_dict': optimizer.state_dict(),\n",
        "         }, os.path.join(checkpoint_path, 'best_model.pth'))\n",
        "    # save freq\n",
        "    if epoch % save_freq == 0:\n",
        "        checkpoint_file = os.path.join(checkpoint_path, f'checkpoint_epoch_{epoch + 1}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoint_file)\n",
        "\n",
        "        # add model\n",
        "        checkpoint_files.append(checkpoint_file)\n",
        "\n",
        "        # if full, delete the old\n",
        "        if len(checkpoint_files) == 3:\n",
        "            os.remove(checkpoint_files.popleft())\n",
        "\n",
        "    # close TensorBoard\n",
        "    writer.close()\n",
        "\n",
        "    # save\n",
        "    torch.save(model.state_dict(), \"csgo_model.pth\")"
      ]
    }
  ]
}